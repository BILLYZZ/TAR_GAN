{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-Supervised Learning NLP- TAR Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#main file for training GAN\n",
    "#Author Bill Zhai, Dec 2019\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "#from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NLP related:\n",
    "vocab_size = 1207\n",
    "embedding_size = 252\n",
    "sequence_length = 252\n",
    "\n",
    "# GAN related:\n",
    "task_name = \"NLP_embedding_\"+str(embedding_size)\n",
    "\n",
    "npSeed = 188\n",
    "np.random.seed(npSeed)\n",
    "x_height, x_width = [sequence_length, embedding_size]\n",
    "num_channels = 1\n",
    "num_classes = 3 #Undamaged, cracked/spalling\n",
    "latent_size = 100\n",
    "labeled_rate = 1.0 #1.0, 0.5, 0.1 this limits the knowledge base of the learning\n",
    "#unlabeled_supp_rate = 0 # percentage of unlabeled data to be supplemented to the learning\n",
    "c_ul = 0\n",
    "# task_path = 'numpyData/BINARY_CR_16_1_split_128_128'\n",
    "# When you wake up, UNCOMMENT THIS line below****\n",
    "task_path = 'numpyData/'\n",
    "# All data (labeled and unlabeled) by class\n",
    "# For NLP: the data input is [[sentence1], [sentence2], ...] where [sentencei] is an array of floats\n",
    "train_data_by_class, test_data_by_class, train_label_by_class, test_label_by_class = [], [], [], []\n",
    "#train_mask_by_class = []\n",
    "#unlabeled_indices_by_class = []\n",
    "labeled_indices_by_class = [] # Marked for labeled data selection to batch\n",
    "\n",
    "train_data_by_class = np.load('numpyData/x_train_by_class.npy', allow_pickle=True)\n",
    "train_label_by_class = np.load('numpyData/y_train_by_class.npy', allow_pickle=True)\n",
    "test_data_by_class = np.load('numpyData/x_test_by_class.npy', allow_pickle=True)\n",
    "test_label_by_class = np.load('numpyData/y_test_by_class.npy', allow_pickle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "unlabeled_data = np.zeros((0, x_width, x_height, num_channels)) # In order to randomly select from\n",
    "# all unlabeled data without considering classes\n",
    "labeled_data_baseline = np.zeros((0, x_width, x_height, num_channels)) # In order to export 0.25 labeled \n",
    "# Load real data:\n",
    "# for i in range(num_classes):\n",
    "#     train_data_by_class.append(np.load(task_path+'/class_'+str(i)+'/trainX.npy'))\n",
    "#     train_label_by_class.append(np.load(task_path+'/class_'+str(i)+'/trainy.npy'))\n",
    "                        \n",
    "#     test_data_by_class.append(np.load(task_path+'/class_'+str(i)+'/testX.npy'))\n",
    "#     test_label_by_class.append(np.load(task_path+'/class_'+str(i)+'/testy.npy'))\n",
    "#     print(\"clas+ \"+str(i)+'has '+str(len(train_label_by_class[i])))\n",
    "\n",
    "    \n",
    "    # Select data as unlabeled:\n",
    "for i in range(num_classes):\n",
    "    numInClass = len(train_label_by_class[i])\n",
    "    #mask =  np.concatenate((np.ones(int(numInClass * labeled_rate), dtype='int'), np.zeros(numInClass - int(numInClass * labeled_rate), dtype='int')), axis=0)\n",
    "    num_unlabeled = int(numInClass-numInClass*labeled_rate)\n",
    "    train_data_class = train_data_by_class[i]\n",
    "    train_label_class = train_data_by_class[i]\n",
    "    indices = [i for i in range(numInClass)]\n",
    "    np.random.shuffle(indices)\n",
    "    unlabeled_is, labeled_is = np.split(indices,[num_unlabeled]) # Unlabeled and labeled indices\n",
    "    \n",
    "    # Concatenate image data array\n",
    "    #unlabeled_data = np.concatenate((unlabeled_data, train_data_by_class[i][unlabeled_is]))\n",
    "    #labeled_data_baseline = np.concatenate((labeled_data_baseline, train_data_by_class[i][labeled_is]))\n",
    "    \n",
    "    #unlabeled_indices_by_class.append(unlabeled_is)\n",
    "    labeled_indices_by_class.append(labeled_is)\n",
    "\n",
    "#np.save(unlabeled_data, 'numpyData/0.25_labeled_data')\n",
    "    \n",
    "numTrain = sum([len(c) for c in train_label_by_class])\n",
    "numTest = sum([len(c) for c in test_label_by_class])\n",
    "log_path = './SSL_GAN_log_' + task_name + '.csv' # Don't worry about log_path, it is named after the task\n",
    "log_path_baseline = './baseline_log.csv'\n",
    "model_path ='./savedModels/GAN'\n",
    "baseline_path = './savedModels/baseline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 252)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_by_class[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 252)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_by_class[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([14, 30,  4, 35,  6, 21,  9, 40, 24, 39, 33, 38, 22, 34, 12, 36,  5,\n",
       "        37, 17,  8, 25, 15, 10, 31,  3, 23, 32, 19, 26, 29, 20, 13, 27,  7,\n",
       "        11, 28, 16,  0,  2,  1, 18]),\n",
       " array([13, 10,  9, 14,  5,  1,  6, 15,  4,  3, 11,  7,  0,  8,  2, 12]),\n",
       " array([16, 12,  2,  7, 21, 20, 18, 17, 14,  0,  8,  4, 19,  6,  5, 15, 10,\n",
       "        11, 13,  1,  9,  3])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_indices_by_class\n",
    "#[array([4594, 4914, 6163, ..., 6263, 8001, 8250]),\n",
    "# array([489, 152, 411, 406, 534,  63, 192, 566, 290,  23, 333, 167, 239,\n",
    " #       506, 569, 475, 217, 491, 574, 473, 232, 530,  88, 183, 520,  99,\n",
    "        #212, 132,   9, 158,  28, 131, 378,   8, 352, 318, 454, 221, 251,\n",
    "        #595, 423, 257, 456, 193, 449, 400, 304,  79, 443, 509,  21, 273,\n",
    "        #325, 197, 432,  97, 415, 485, 209, 447, 103, 327, 130, 260, 572,\n",
    "        #162, 386,  49,  13, 399, 402, 151, 419, 294,  95, 396, 561,  65,\n",
    "        #236, 271, 554, 502, 312, 436,  48, 341, 548, 227, 585, 544, 551,\n",
    "        #102, 145, 274, 384, 155, 570, 435, 316, 442, 445, 267, 543, 490,\n",
    "        # 89, 300, 459, 556, 185, 106, 278, 109, 173,  19, 138, 366, 182,\n",
    "       # 537, 289, 559,  70, 247,  82, 287, 198,  68, 383, 481,  87, 218,\n",
    "       # 549, 263, 523, 526, 538, 101, 137, 482, 434, 511,  62, 465,  60,\n",
    "        #597,  40, 361,  46, 464, 552, 345])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    # normalize data\n",
    "#     x /= 255.0\n",
    "    x = (x - 127.5) / 127.5\n",
    "    return x.reshape((-1, x_height, x_width, 3)) #x is 4 dimensional-- (num_images, height, width, depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save masked/labeled image to numpy array\n",
    "def save_masked(toFolder):\n",
    "    for i in range(num_classes):\n",
    "        if not os.path.exists('numpyData/'+toFolder+'/class_'+str(i)):\n",
    "            os.makedirs('numpyData/'+toFolder+'/class_'+str(i))\n",
    "        np.save('numpyData/'+toFolder+'/class_'+str(i)+'/trainX', train_data_by_class[i][train_mask_by_class[i].astype(bool)])\n",
    "        np.save('numpyData/'+toFolder+'/class_'+str(i)+'/trainy', train_label_by_class[i][train_mask_by_class[i].astype(bool)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_masked(\"labeled_for_cnn_seed_\"+str(npSeed)+'_rate_'+str(labeled_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#shuffle data array, labels array, and labeledMask array, each image's properties should remain consistant (labeled/unlabeled)\n",
    "#during the whole experiment. Instead of assigning labeled mask per next batch, it should be globally defined\n",
    "#prior to running the experiment --BZ, August, 2019\n",
    "def shuffle_data(data, labels, labeledMask):#all arrays here are row vectors? labels and data are columnwise\n",
    "    #np.random.seed(123)#for debugging purpose\n",
    "    indices = np.arange(labels.shape[0]) #index sequence whose length = len(labels)\n",
    "    np.random.shuffle(indices) #In place\n",
    "    shuffled_indices = indices #Useless assignment for clarity- shuffle is in place\n",
    "    if labeledMask is None:#for cases used by get_test_batch() function--BZ, August, 2019\n",
    "        return data[shuffled_indices], labels[shuffled_indices]\n",
    "    else:\n",
    "        return data[shuffled_indices], labels[shuffled_indices], labeledMask[shuffled_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#new batch functions:--BZ, August, 2019\n",
    "#because labeled mask should stay with the same images after each shuffle (which happens at the start of each new epoch),\n",
    "#get_batch() and get_labeled_mask() should be merged into one function\n",
    "def get_training_batch_and_labeled_mask(XTrain, yTrain, labeledMask, batchSize):\n",
    "    #first shuffle the indices:\n",
    "    XTrainRandom, yTrainRandom, labeledMaskRandom = shuffle_data(XTrain, yTrain, labeledMask);\n",
    "    #a generator that slices and returns batchSize of XTrain and yTrain instances from top down\n",
    "    counter = 0;\n",
    "    while True:\n",
    "        if counter >= len(yTrain):\n",
    "            break;\n",
    "        returnXTrain = XTrainRandom[counter:counter + batchSize];\n",
    "        returnYTrain = yTrainRandom[counter:counter + batchSize];\n",
    "        returnLabeledMask = labeledMaskRandom[counter:counter + batchSize];\n",
    "        counter = counter + batchSize;\n",
    "        yield returnXTrain, returnYTrain, returnLabeledMask\n",
    "def get_test_batch(XTest, yTest, batchSize): #essentially the same function as above, restated here for explicity\n",
    "    #first shuffle the indices:\n",
    "    XTestRandom, yTestRandom = shuffle_data(XTest, yTest, None);\n",
    "    #a generator that slices and returns batchSize of XTrain and yTrain instances from top down\n",
    "    counter = 0;\n",
    "    while True:\n",
    "        if counter >= len(yTest):\n",
    "            break;\n",
    "        returnXTest = XTestRandom[counter:counter + batchSize];\n",
    "        returnYTest = yTestRandom[counter:counter + batchSize];\n",
    "        counter = counter + batchSize;\n",
    "        yield returnXTest, returnYTest\n",
    "\n",
    "def get_train_batch(XTrain, yTrain, batchSize):\n",
    "    get_test_batch(XTrain, yTrain, batchSize)\n",
    "        \n",
    "def get_balance_train_batch(train_data_by_class, train_label_by_class, train_mask_by_class, batchSize):\n",
    "    numEachClass = int(np.floor(batchSize / num_classes))\n",
    "    returnData = np.zeros((0, x_width, x_height, num_channels))\n",
    "    returnLabel = np.zeros((0, num_classes))\n",
    "    returnLabeledMask = np.zeros((0))\n",
    "    for i in range(num_classes):\n",
    "        train_data = train_data_by_class[i]\n",
    "        train_label = train_label_by_class[i]\n",
    "        train_mask = train_mask_by_class[i]\n",
    "        indices = [v for v in range(len(train_label))]\n",
    "        np.random.shuffle(indices)\n",
    "        selectIndices = indices[0:numEachClass]\n",
    "        returnData = np.concatenate((returnData, train_data[selectIndices]))\n",
    "        returnLabel = np.concatenate((returnLabel, train_label[selectIndices]))\n",
    "        returnLabeledMask = np.concatenate((returnLabeledMask, train_mask[selectIndices]))\n",
    "    return returnData, returnLabel, returnLabeledMask\n",
    "\n",
    "def get_balance_train_batch_2(train_data_by_class, train_label_by_class, train_mask_by_class, batchSize):\n",
    "    numEachClass = int(batchSize / num_classes)\n",
    "    numEachClass_unlabeled_labeled = [numEachClass - int(numEachClass*labeled_rate), int(numEachClass*labeled_rate)]\n",
    "#     print(numEachClass)\n",
    "#     print(numEachClass_unlabeled_labeled)\n",
    "    returnData = np.zeros((0, x_width, x_height, num_channels))\n",
    "    returnLabel = np.zeros((0, num_classes))\n",
    "    returnLabeledMask = np.zeros((0))\n",
    "    for i in range(num_classes):\n",
    "        for j in [0,1]: #for unlabeled and labeled\n",
    "            indices = []\n",
    "            for k in range(len(train_mask_by_class[i])):\n",
    "                if train_mask_by_class[i][k] == j:\n",
    "                    indices.append(k)\n",
    "#             print('label status_'+str(j))\n",
    "#             print(indices)\n",
    "            np.random.shuffle(indices)\n",
    "            selectIndices = indices[0:numEachClass_unlabeled_labeled[j]]\n",
    "#             print('selected')\n",
    "#             print(selectIndices)\n",
    "            train_data = train_data_by_class[i]\n",
    "            train_label = train_label_by_class[i]\n",
    "            train_mask = train_mask_by_class[i]\n",
    "            \n",
    "            returnData = np.concatenate((returnData, train_data[selectIndices]))\n",
    "            returnLabel = np.concatenate((returnLabel, train_label[selectIndices]))\n",
    "            returnLabeledMask = np.concatenate((returnLabeledMask, train_mask[selectIndices]))\n",
    "    return returnData, returnLabel, returnLabeledMask\n",
    "\n",
    "# Input: c_ul is the relative portion ratio related to each class in a batch\n",
    "def get_balance_train_batch_3(train_data_by_class, train_label_by_class, batchSize, c_ul):\n",
    "    numEachPortion = int(batchSize/(num_classes+1+c_ul)) # One portion of fake,c_ul portion of unlabeled\n",
    "    returnData = np.zeros((0, sequence_length), dtype='int')\n",
    "    returnLabel = np.zeros((0, num_classes))\n",
    "    returnLabeledMask = np.zeros((0))\n",
    "    # First load labeled Data (mask is 1)\n",
    "    for i in range(num_classes):\n",
    "        labeled_is = np.random.choice(labeled_indices_by_class[i], numEachPortion)\n",
    "        returnData = np.concatenate((returnData, train_data_by_class[i][labeled_is]))\n",
    "        returnLabel = np.concatenate((returnLabel, train_label_by_class[i][labeled_is]))\n",
    "        returnLabeledMask = np.concatenate((returnLabeledMask, np.ones(numEachPortion))) # 1 for labeled\n",
    "    # Then load unlabeled Data (mask is 0)\n",
    "    if len(unlabeled_data) is not 0:\n",
    "        num_unlabeled = numEachPortion * c_ul\n",
    "        unlabeled_is = np.random.choice([i for i in range(len(unlabeled_data))], num_unlabeled)\n",
    "        returnData = np.concatenate((returnData, unlabeled_data[unlabeled_is]))\n",
    "        returnLabel = np.concatenate((returnLabel, np.zeros((num_unlabeled, num_classes))))\n",
    "        returnLabeledMask = np.concatenate((returnLabeledMask, np.zeros(num_unlabeled))) # 0 for unlabeled\n",
    "    return returnData, returnLabel, returnLabeledMask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_batch(test_data_by_class, test_label_by_class, batchSize):\n",
    "    XTest = np.zeros((0, sequence_length), dtype='int')\n",
    "    yTest = np.zeros((0, num_classes))\n",
    "    for i in range(num_classes):\n",
    "        XTest = np.concatenate((XTest, test_data_by_class[i]))\n",
    "        yTest = np.concatenate((yTest, test_label_by_class[i]))\n",
    "    #first shuffle the indices:\n",
    "    XTestRandom, yTestRandom = shuffle_data(XTest, yTest, None);\n",
    "    #a generator that slices and returns batchSize of XTrain and yTrain instances from top down\n",
    "    counter = 0;\n",
    "    while True:\n",
    "        if counter >= len(yTest):\n",
    "            break;\n",
    "        returnXTest = XTestRandom[counter:counter + batchSize];\n",
    "        returnYTest = yTestRandom[counter:counter + batchSize];\n",
    "        counter = counter + batchSize;\n",
    "        yield returnXTest, returnYTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 287,  306, 1119, ...,    1,    1,    1],\n",
       "       [ 144, 1040,  995, ...,    1,    1,    1],\n",
       "       [ 281, 1047,  568, ...,    1,    1,    1],\n",
       "       ...,\n",
       "       [ 287,  796,  131, ...,    1,    1,    1],\n",
       "       [ 264, 1040,  459, ...,    1,    1,    1],\n",
       "       [ 306,  251, 1060, ...,    1,    1,    1]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_by_class[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, label, mask = get_balance_train_batch_2(train_data_by_class, train_label_by_class, train_mask_by_class, 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def D(data_source, x_real, x_fake, dropout_rate, is_training, reuse = True, print_summary = True):\n",
    "    # data_source is a string, either \"fake\" or \"real\", which determines whether do to the word\n",
    "    # embedding lookup to avoid non-differentiability issues.\n",
    "    # discriminator (x -> n + 1 class)\n",
    "\n",
    "    with tf.variable_scope('Discriminator', reuse = reuse) as scope:\n",
    "        # Embedding layer\n",
    "        # Input x has shape [batch_size, 63] where 63 is the sequence length\n",
    "        W_embed = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0), name=\"W_embed\")\n",
    "        embedded_chars = tf.nn.embedding_lookup(W_embed, x_real)\n",
    "        # Add a channel dimension:\n",
    "        embedded_char_expanded = tf.expand_dims(embedded_chars, -1) \n",
    "        # Output size: [batch_size, sequence_length, embedding_size, 1]\n",
    "        \n",
    "        print('fake shape is!')\n",
    "        print(x_fake.get_shape())\n",
    "        print('embed_char_expand shape is!')\n",
    "        print(embedded_char_expanded.get_shape())\n",
    "        # conditional pipeline!\n",
    "        def f1(): return embedded_char_expanded\n",
    "        def f2(): return x_fake\n",
    "        real_or_fake = tf.math.equal('real', data_source)\n",
    "        input_x = tf.cond(real_or_fake, f1, f2)\n",
    "        \n",
    "        print('input_x shape is!')\n",
    "        print(input_x.get_shape())\n",
    "        #assert input_x.get_shape()[1:] == [x_height, x_width, num_channels]\n",
    "        # layer1 - do not use Batch Normalization on the first layer of Discriminator\n",
    "        conv1 = tf.layers.conv2d(input_x, 32, [3, 3],\n",
    "                                 strides = [2, 2],\n",
    "                                 padding = 'same')\n",
    "        lrelu1 = tf.maximum(0.2 * conv1, conv1) #leaky relu\n",
    "        dropout1 = tf.layers.dropout(lrelu1, dropout_rate)\n",
    "\n",
    "        # layer2\n",
    "        conv2 = tf.layers.conv2d(dropout1, 64, [3, 3],\n",
    "                                 strides = [2, 2],\n",
    "                                 padding = 'same')\n",
    "        batch_norm2 = tf.layers.batch_normalization(conv2, training = is_training, momentum=0.8)\n",
    "        lrelu2 = tf.maximum(0.2 * batch_norm2, batch_norm2)\n",
    "        dropout2 = tf.layers.dropout(lrelu2, dropout_rate)\n",
    "\n",
    "        # layer3\n",
    "        conv3 = tf.layers.conv2d(lrelu2, 64, [3, 3],\n",
    "                                 strides = [1, 1],\n",
    "                                 padding = 'same')\n",
    "        batch_norm3 = tf.layers.batch_normalization(conv3, training = is_training)\n",
    "        lrelu3 = tf.maximum(0.2 * batch_norm3, batch_norm3)\n",
    "        dropout3 = tf.layers.dropout(lrelu3, dropout_rate)\n",
    "        #dropout3 = tf.layers.dropout(lrelu3, dropout_rate)\n",
    "        #dropout3 = tf.layers.dropout(lrelu2, dropout_rate)\n",
    "        # layer 4\n",
    "        #conv4 = tf.layers.conv2d(dropout3, 256, [3, 3],\n",
    "                                #strides = [1, 1],\n",
    "                                #padding = 'same')\n",
    "        # do not use batch_normalization on this layer - next layer, \"flatten5\",\n",
    "        # will be used for \"Feature Matching\"\n",
    "        #lrelu4 = tf.maximum(0.2 * conv4, conv4)\n",
    "\n",
    "        # layer 5\n",
    "        flatten_length = dropout3.get_shape().as_list()[1] * \\\n",
    "                         dropout3.get_shape().as_list()[2] * dropout3.get_shape().as_list()[3]\n",
    "        flatten5 = tf.reshape(dropout3, (-1, flatten_length)) # used for \"Feature Matching\" \n",
    "        fc5 = tf.layers.dense(flatten5, (num_classes + 1))\n",
    "        output = tf.nn.softmax(fc5, name=\"D_output\")\n",
    "        \n",
    "        assert output.get_shape()[1:] == [num_classes + 1]\n",
    "\n",
    "        if print_summary:\n",
    "            print('Discriminator summary:\\n x: %s\\n' \\\n",
    "                  ' D1: %s\\n D2: %s\\n D3: %s\\n D4: %s\\n' %(x.get_shape(), \n",
    "                                                           dropout1.get_shape(),\n",
    "                                                           lrelu2.get_shape(), \n",
    "                                                           dropout3.get_shape(),\n",
    "                                                           lrelu4.get_shape()))\n",
    "        #return flatten5, fc5, output\n",
    "        #debug: return each layer's output --BZ Nov 28, 2019\n",
    "        return flatten5, fc5, output, real_or_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def G(z, is_training, reuse = False, print_summary = False):\n",
    "    # generator (z -> x)\n",
    "\n",
    "    with tf.variable_scope('Generator', reuse = reuse) as scope:\n",
    "        #z is 100*1\n",
    "        fc1 = tf.layers.dense(z, 63*63*128)\n",
    "        # layer 0\n",
    "        z_reshaped = tf.reshape(fc1, [-1, 63, 63, 128])\n",
    "\n",
    "    \n",
    "        \n",
    "        # layer 1\n",
    "        deconv1 = tf.layers.conv2d_transpose(z_reshaped,\n",
    "                                             filters = 128,\n",
    "                                             kernel_size = [3, 3],\n",
    "                                             strides = [2, 2],\n",
    "                                             padding = 'same')\n",
    "        batch_norm1 = tf.layers.batch_normalization(deconv1, training = is_training, momentum=0.8)\n",
    "        relu1 = tf.nn.relu(batch_norm1)\n",
    "        #64*64*64\n",
    "        # layer 2\n",
    "        deconv2 = tf.layers.conv2d_transpose(relu1,\n",
    "                                             filters = 64,\n",
    "                                             kernel_size = [3, 3],\n",
    "                                             strides = [2, 2],\n",
    "                                             padding = 'same')\n",
    "        batch_norm2 = tf.layers.batch_normalization(deconv2, training = is_training, momentum=0.8)\n",
    "        relu2 = tf.nn.relu(batch_norm2)\n",
    "        #128*128*3\n",
    "        # layer 3\n",
    "        #deconv3 = tf.layers.conv2d_transpose(relu2,\n",
    "                                             #filters = 64,\n",
    "                                             #kernel_size = [3, 3],\n",
    "                                             #strides = [1, 1],\n",
    "                                             #padding = 'same')\n",
    "        #batch_norm3 = tf.layers.batch_normalization(deconv3, training = is_training)\n",
    "        #relu3 = tf.nn.relu(batch_norm3)\n",
    "\n",
    "        # layer 4 - do not use Batch Normalization on the last layer of Generator\n",
    "        deconv4 = tf.layers.conv2d_transpose(relu2,\n",
    "                                             filters = num_channels,\n",
    "                                             kernel_size = [3, 3],\n",
    "                                             strides = [1, 1],\n",
    "                                             padding = 'same')\n",
    "        tanh4 = tf.tanh(deconv4, name=\"G_output\")\n",
    "        print('tanh shape')\n",
    "        print(tanh4.get_shape())\n",
    "        assert tanh4.get_shape()[1:] == [x_height, x_width, num_channels]\n",
    "        \n",
    "#         decoded_sentences_list = [] # A list of sentences [batch_size, seq_length]\n",
    "#         for d in deconv4: # For each image in batch\n",
    "#             decoded_word_list = []\n",
    "#             for word in d: # For each sentence in sentence image\n",
    "#                 word_reduce = tf.squeeze(word, axis=1) # take out the channel dimension\n",
    "#                 # Iterate each row of W_embed and find the row with the closest vector distance\n",
    "#                 smallest_norm = np.inf\n",
    "#                 smallest_norm_id = None\n",
    "#                 for r in len(W_embed):\n",
    "#                     norm = tf.norm(tanh4[d], word_reduce)\n",
    "#                     if norm <= smallest_norm:\n",
    "#                         smallest_norm = norm\n",
    "#                         smallest_norm_id = r\n",
    "#                 decoded_word_id = smallest_norm_id # To avoid confusion\n",
    "#                 decoded_word_list.append(decoded_word_id)\n",
    "#             decoded_sentences_list.append(decoded_word_list)\n",
    "#         assert len(decoded_sentences_list[0]) == seq_length\n",
    "        \n",
    "\n",
    "        if print_summary:\n",
    "            print('Generator summary:\\n z: %s\\n' \\\n",
    "                  ' G0: %s\\n G1: %s\\n G2: %s\\n G3: %s\\n G4: %s\\n' %(z.get_shape(),\n",
    "                                                                    z_reshaped.get_shape(),\n",
    "                                                                    relu1.get_shape(),\n",
    "                                                                    relu2.get_shape(),\n",
    "                                                                    relu3.get_shape(),\n",
    "                                                                    tanh4.get_shape()))\n",
    "        return tanh4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#build model for each batch using D() and G() functions\n",
    "def build_model(x_real, z, label, dropout_rate, is_training, print_summary = False):\n",
    "    # build model\n",
    "    #generate fake images\n",
    "    x_fake = G(z, is_training, reuse = False, print_summary = print_summary)\n",
    "    \n",
    "    \n",
    "    #Discriminator on real data (labeled and unlabeled)  flatten5, fc5, output\n",
    "    D_real_features, D_real_logit, D_real_prob, rf_real = D(tf.constant('real', dtype=tf.string), x_real, x_fake, dropout_rate, is_training,\n",
    "                                                   reuse = False, print_summary = print_summary)\n",
    "    \n",
    "    #Discriminator for fake images\n",
    "    D_fake_features, D_fake_logit, D_fake_prob, rf_fake = D(tf.constant('fake', dtype=tf.string), x_real, x_fake, dropout_rate, is_training,\n",
    "                                                   reuse = True, print_summary = print_summary)\n",
    "\n",
    "    return D_real_features, D_real_logit, D_real_prob, D_fake_features, D_fake_logit, D_fake_prob, x_fake, rf_real, rf_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model that only contains the discriminator for production model\n",
    "def build_model_production(x_real, label, dropout_rate, is_training, print_summary = False):\n",
    "    x, dropout1, batch_norm2, dropout3, dropout4, D_real_features, D_real_logit, D_real_prob, conv1 = D(x_real, dropout_rate, is_training,\n",
    "                                                   reuse = False, print_summary = print_summary)\n",
    "    correct_prediction = tf.equal(tf.argmax(D_real_prob[:, :-1], 1),#arg max returns the indices--Bill Zhai Aug, 2019\n",
    "                                  tf.argmax(label, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) \n",
    "    return x, dropout1, batch_norm2, dropout3, dropout4, D_real_features, D_real_logit, D_real_prob, accuracy, correct_prediction, conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def prepare_labels(label):\n",
    "    # add extra label for telling apart fake data from real data\n",
    "    #essentially appending another column to the very end of the matrix which is for the 'fake' class\n",
    "    #uses one hot, so this newly appended column is all zeros--Bill Zhai Aug, 2019\n",
    "    extended_label = tf.concat([label, tf.zeros([tf.shape(label)[0], 1])], axis = 1)\n",
    "\n",
    "    return extended_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def loss_accuracy(D_real_features, D_real_logit, D_real_prob, D_fake_features,\n",
    "                  D_fake_logit, D_fake_prob, extended_label, labeled_mask):\n",
    "    epsilon = 1e-8 # used to avoid NAN loss\n",
    "    # *** Discriminator loss ***\n",
    "    # supervised loss\n",
    "    # which class the real data belongs to\n",
    "    tmp = tf.nn.softmax_cross_entropy_with_logits(logits = D_real_logit,#cross_entrypy_with_logits is the only function available\n",
    "                                                  labels = extended_label)\n",
    "    #question: is this an over simplification?--no, becuase tmp is a vector: num_samples_in_batch * 1\n",
    "    D_L_supervised = tf.reduce_sum(labeled_mask * tmp) / (tf.reduce_sum(labeled_mask)+1e-6) # to ignore unlabeled data\n",
    "                                                                                     \n",
    "\n",
    "    # unsupervised loss\n",
    "    # data is real\n",
    "    prob_real_be_real = 1 - D_real_prob[:, -1] + epsilon #\"-1\" signifies the last column which is probabilities of being \"fake\"\n",
    "    tmp_log = tf.log(prob_real_be_real)\n",
    "    D_L_unsupervised1 = -1 * tf.reduce_mean(tmp_log)\n",
    "\n",
    "    # data is fake\n",
    "    prob_fake_be_fake = D_fake_prob[:, -1] + epsilon\n",
    "    tmp_log = tf.log(prob_fake_be_fake)\n",
    "    D_L_unsupervised2 = -1 * tf.reduce_mean(tmp_log)\n",
    "\n",
    "    D_L = D_L_supervised + D_L_unsupervised1 + D_L_unsupervised2\n",
    "\n",
    "    # *** Generator loss ***\n",
    "    # fake data is mistaken to be real\n",
    "    #prob_fake_be_real = 1 - D_fake_prob[:, -1] + epsilon\n",
    "    #tmp_log =  tf.log(prob_fake_be_real)\n",
    "    #G_L1 = -1 * tf.reduce_mean(tmp_log)\n",
    "    G_L1 = 0.0 # Due to non-differentiable decoding\n",
    "\n",
    "    # Feature Maching\n",
    "    tmp1 = tf.reduce_mean(D_real_features, axis = 0)\n",
    "    tmp2 = tf.reduce_mean(D_fake_features, axis = 0)\n",
    "    G_L2 = tf.reduce_mean(tf.square(tmp1 - tmp2))\n",
    "\n",
    "    G_L = G_L1 + G_L2\n",
    "\n",
    "    # accuracy--This is cross validation accuracy within the training set--Nov, 2019\n",
    "    correct_prediction = tf.equal(tf.argmax(D_real_prob[:, :-1], 1),#arg max returns the indices--Bill Zhai Aug, 2019\n",
    "                                  tf.argmax(extended_label[:, :-1], 1), name='correct_prediction')\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) #cast boolean to float32--Bill Zhai Aug, 2019\n",
    "    \n",
    "    \n",
    "    return D_L_supervised, D_L_unsupervised1, D_L_unsupervised2, D_L, G_L, accuracy, correct_prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def optimizer(D_Loss, G_Loss, D_learning_rate, G_learning_rate):\n",
    "    # D and G optimizer\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(extra_update_ops):\n",
    "        all_vars = tf.trainable_variables()\n",
    "        D_vars = [var for var in all_vars if var.name.startswith('Discriminator')]\n",
    "        G_vars = [var for var in all_vars if var.name.startswith('Generator')]\n",
    "        #print('D_vars:')\n",
    "        #print(D_vars)\n",
    "        D_optimizer = tf.train.AdamOptimizer(D_learning_rate).minimize(D_Loss, var_list = D_vars)\n",
    "        G_optimizer = tf.train.AdamOptimizer(G_learning_rate).minimize(G_Loss, var_list = G_vars)\n",
    "        return D_optimizer, G_optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Assume len(data) > 5 --BZ Nov 30, 2019\n",
    "\n",
    "def plot_fake_data(data, epoch):\n",
    "    # visualize some data generated by G\n",
    "    data = (1/2.5) * data + 0.5\n",
    "    fig, axs = plt.subplots(len(data), figsize=(30,30))\n",
    "    cnt = 0\n",
    "    for j in range(len(data)):\n",
    "        #print(j)\n",
    "        #print(data[cnt, :, :, :])\n",
    "        axs[j].imshow(data[cnt, :, :, :])\n",
    "        axs[j].axis(\"off\")\n",
    "        cnt = cnt + 1\n",
    "    print('graphed!')        \n",
    "    if not os.path.exists(\"./training_fake_figure\"):\n",
    "        os.mkdir(\"./training_fake_figure\");\n",
    "    plt.savefig(\"training_fake_figure/%d.jpg\" % epoch)\n",
    "    plt.close()\n",
    "\n",
    "def save_fake_image(data, epoch):\n",
    "    #if not os.path.exists(\"./training_fake_imageMatrix\"):\n",
    "        #os.mkdir(\"./training_fake_imageMatrix\");\n",
    "    #np.save(\"./training_fake_imageMatrix/epoch_\"+str(epoch), data)\n",
    "    #only plot the last image of the batch\n",
    "    if not os.path.exists(\"./training_fake_figure\"):\n",
    "        os.mkdir(\"./training_fake_figure\")\n",
    "    plt.imshow(data[-1]/2+0.5)\n",
    "    plt.set_cmap('hot')\n",
    "    plt.axis('off')\n",
    "    #print(data[-1])\n",
    "    plt.savefig(\"training_fake_figure/%d.jpg\" % epoch)\n",
    "    plt.close()\n",
    "\n",
    "def save_fake_sentence(data, epoch, w_embed): # logmode can be 'a' for 'w' (append or overwrite)\n",
    "    if not os.path.exists(\"./training_fake_sentence\"):\n",
    "        os.mkdir(\"./training_fake_figure\")\n",
    "    if epoch == 0:\n",
    "        logmode = 'w'\n",
    "    else:\n",
    "        logmode = 'a'\n",
    "    \n",
    "    with open(file_path, logmode) as f: # start overwriting\n",
    "        if epoch == 0:\n",
    "            header = 'epoch, train_loss_D, train_loss_G,' \\\n",
    "                         'train_Acc, val_Acc, recall0, recall1, recall2\\n'\n",
    "            f.write(header)\n",
    "        \n",
    "        # From numbers back to sentences\n",
    "        w_embed_transpose = np.transpose(w_embed)\n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        tf.nn.embedding_lookup(w_embed, ids, max_norm=None, name=None)\n",
    "        line = '%d, %f, %f, %f, %f, %f, %f, %f\\n' %(epoch, train_loss_D, train_loss_G, train_Acc,\n",
    "                                                cv_Acc, recall0, recall1, recall2)\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def save_model_on_improvement(file_path, sess, cv_acc, cv_accs):\n",
    "  #  # save model when there is improvemnet in cv_acc value\n",
    "    if cv_accs == [] or cv_acc >= np.max(cv_accs):\n",
    "        saver = tf.train.Saver(max_to_keep = 1)\n",
    "        saver.save(sess, file_path)\n",
    "        print('Model saved')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the top 5 best model based on validation accuracy\n",
    "def save_model_top_five(folder_path, sess, cv_acc, cv_accs):\n",
    "    #cv_acc is inside cv_accs\n",
    "    if not os.path.exists(folder_path+'/'+task_name):\n",
    "        os.mkdir(folder_path+'/'+task_name)\n",
    "    sortedAccs = np.sort(cv_accs)\n",
    "    for i in range(len(cv_accs)):\n",
    "        if i >= 5:\n",
    "            return\n",
    "        if cv_acc >= sortedAccs[i]:\n",
    "            saver = tf.train.Saver(max_to_keep = 1)\n",
    "            saver.save(sess, folder_path+'/'+task_name+'/'+'_top_'+str(i+1)+'_SSL_GAN.ckpt')\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model based on TPR and TNR criteria\n",
    "def save_model_TPR_TNR(folder_path, sess, epoch, cv_acc, cv_accs, TPR, TNR, TPRs):\n",
    "    if not os.path.exists(folder_path+'/'+task_name):\n",
    "        os.mkdir(folder_path+'/'+task_name)\n",
    "    sortedTPRs = np.sort(TPRs)[::-1] # Sort in Descending Order!\n",
    "    for i in range(len(sortedTPRs)):\n",
    "        if i >= 5:\n",
    "            return\n",
    "        #if TPR >= sortedTPRs[i] and TPR >= 0.88 and TNR >= 0.91:\n",
    "        if TPR >= sortedTPRs[i]:\n",
    "            print('save model')\n",
    "            saver = tf.train.Saver(max_to_keep = 1)\n",
    "            saver.save(sess, folder_path+'/'+task_name+'/'+'_TPR_top_'+str(i+1)+'_epoch_'+str(epoch)+'_SSL_GAN.ckpt')\n",
    "            return\n",
    "        \n",
    "# Save model based on TPR and TNR criteria\n",
    "def save_model_recalls(folder_path, sess, epoch, cv_acc, cv_accs, recall0, recall1, recall2):\n",
    "    if not os.path.exists(folder_path+'/'+task_name):\n",
    "        os.mkdir(folder_path+'/'+task_name)\n",
    "    if recall0 > 0.5 and recall2 > 0.5:\n",
    "        saver = tf.train.Saver(max_to_keep = 1)\n",
    "        saver.save(sess, folder_path+'/'+task_name+'/'+'recall0_'+str(recall0)+'recall2_'+str(recall2)+'_epoch_'+str(epoch)+'_SSL_GAN.ckpt')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(file_path, sess):\n",
    "    saver = tf.train.Saver(max_to_keep = 1)\n",
    "    saver.save(sess, file_path)\n",
    "    print('Every 500 model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def log_loss_acc_binary(file_path, epoch, train_loss_D, train_loss_G, train_Acc,\n",
    "                 cv_Acc, recall0, recall1, recall2, log_mode):\n",
    "    # log train and cv losses as well as accuracy\n",
    "    mode = log_mode if epoch == 0 else 'a'\n",
    "\n",
    "    with open(file_path, mode) as f:\n",
    "        if mode == 'w':\n",
    "            header = 'epoch, train_loss_D, train_loss_G,' \\\n",
    "                     'train_Acc, val_Acc, recall0, recall1, recall2\\n'\n",
    "            f.write(header)\n",
    "\n",
    "        line = '%d, %f, %f, %f, %f, %f, %f, %f\\n' %(epoch, train_loss_D, train_loss_G, train_Acc,\n",
    "                                                cv_Acc, recall0, recall1, recall2)\n",
    "        f.write(line)\n",
    "def log_loss_acc_ternary(file_path, epoch, train_loss_D, train_loss_G, train_Acc,\n",
    "                 cv_Acc, cm00, cm01, cm02, cm10, cm11, cm12, cm20, cm21, cm22, log_mode):\n",
    "    # log train and cv losses as well as accuracy\n",
    "    mode = log_mode if epoch == 0 else 'a'\n",
    "\n",
    "    with open(file_path, mode) as f:\n",
    "        if mode == 'w':\n",
    "            header = 'epoch, train_loss_D, train_loss_G,' \\\n",
    "                     'train_Acc, val_Acc, recall0, recall1, recall2\\n'\n",
    "            f.write(header)\n",
    "\n",
    "        line = '%d, %f, %f, %f, %f, %f, %f, %f, %f, %f, %f, %f, %f, %f\\n' %(epoch, train_loss_D, train_loss_G, train_Acc,\n",
    "                                                cv_Acc, cm00, cm01, cm02, cm10, cm11, cm12, cm20, cm21, cm22)\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Modified by BZ on Nov 3, 2019\n",
    "#Note: predictions vector has binary entries: 1 corresponds to correct prediction, 0 is wrong prediction\n",
    "def compute_val_accuracy(correct_predictions):\n",
    "    return np.sum(correct_predictions)/len(correct_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#training function--Bill Zhai Aug 2019\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_SSL_GAN(batch_size, num_epochs, train_data_by_class, train_label_by_class, test_data_by_class, test_label_by_class):\n",
    "    tf.disable_eager_execution()\n",
    "    # train Semi-Supervised Learning GAN\n",
    "    train_D_losses, train_G_losses, train_Accs = [], [], []\n",
    "    val_D_losses, val_G_losses, val_Accs, TPRs = [], [], [], []\n",
    "    \n",
    "    cv_size = batch_size\n",
    "    num_train_exs = numTrain\n",
    "    num_val_exs = numTest\n",
    "    print(batch_size)\n",
    "    print(\"num_train_exs: \", num_train_exs)\n",
    "    print(\"num_val_exs: \", num_val_exs)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    x = tf.placeholder(tf.int32, name = 'x', shape = [None, sequence_length])\n",
    "    label = tf.placeholder(tf.float32, name = 'label', shape = [None, num_classes]) # one hot label--BZ, August, 2019\n",
    "    labeled_mask = tf.placeholder(tf.float32, name = 'labeled_mask', shape = [None])\n",
    "    z = tf.placeholder(tf.float32, name = 'z', shape = [None, latent_size])#one 1-d noise vector per training example\n",
    "    dropout_rate = tf.placeholder(tf.float32, name = 'dropout_rate')\n",
    "    is_training = tf.placeholder(tf.bool, name = 'is_training')\n",
    "    G_learning_rate = tf.placeholder(tf.float32, name = 'G_learning_rate')\n",
    "    D_learning_rate = tf.placeholder(tf.float32, name = 'D_learning_rate')\n",
    "\n",
    "    model = build_model(x, z, label, dropout_rate, is_training, print_summary = False)\n",
    "    #    return D_real_features, D_real_logit, D_real_prob, D_fake_features, D_fake_logit, D_fake_prob\n",
    "    D_real_features, D_real_logit, D_real_prob, D_fake_features, D_fake_logit, D_fake_prob, fake_data,\\\n",
    "    rf_real, rf_fake = model\n",
    "    extended_label = prepare_labels(label) #is only for real image data\n",
    "    loss_acc  = loss_accuracy(D_real_features, D_real_logit, D_real_prob,\n",
    "                              D_fake_features, D_fake_logit, D_fake_prob,\n",
    "                              extended_label, labeled_mask)\n",
    "    _, _, _, D_L, G_L, accuracy, correct_prediction = loss_acc\n",
    "    D_optimizer, G_optimizer = optimizer(D_L, G_L, G_learning_rate, D_learning_rate)\n",
    "\n",
    "    \n",
    "#     validation_generator = get_batch(data_path, label_path, num_val_exs, num_train_exs, True)\n",
    "\n",
    "    print('training....')\n",
    "\n",
    "    with tf.Session() as sess:       \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        #mnist_set = get_data()\n",
    "\n",
    "        t_total = 0\n",
    "        #changed to iterating on number of epochs!\n",
    "        iter_count = 0\n",
    "        iter_since_last_val = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            for iteration in range(int(numTrain/(batch_size*(num_classes+c_ul)/(num_classes+1+c_ul)))):\n",
    "            #batch_num = 0 #added--BZ Nov 2, 2019\n",
    "            #training_generator = get_training_batch_and_labeled_mask(X_train, y_train, LABELED_MASK, batch_size);\n",
    "                train_batch_x, train_batch_y, train_batch_mask = get_balance_train_batch_3(train_data_by_class, train_label_by_class, batch_size, c_ul)            #for train_batch_x, train_batch_y, train_batch_mask in training_generator:\n",
    "                t_start = time.time()\n",
    "                #batch_z = np.random.uniform(-1.0, 1.0, size = (batch_size, latent_size)) #\n",
    "                #batch_z = np.random.normal(0, 1, size = (int(batch_size/num_classes), latent_size))\n",
    "                batch_z = np.random.normal(0, 1, size = (int(batch_size/(num_classes+1+c_ul)), latent_size))\n",
    "                #function is to be modified as labeled mask should stay with \"labeled\" images which are shuffled BZ--August, 2019\n",
    "                #mask = get_training_batch_and_labeled_mask(XTrain, yTrain, labeledMask, batchSize);--marked as solved BZ, Nov, 2019\n",
    "                train_feed_dictionary = {x: train_batch_x,\n",
    "                                         z: batch_z,\n",
    "                                         label: train_batch_y,\n",
    "                                         labeled_mask: train_batch_mask,\n",
    "                                         dropout_rate: 0.25,\n",
    "                                         G_learning_rate: 2e-5,\n",
    "                                         D_learning_rate: 2e-5,\n",
    "                                         is_training: True}\n",
    "\n",
    "                D_optimizer.run(feed_dict = train_feed_dictionary)\n",
    "                G_optimizer.run(feed_dict = train_feed_dictionary)\n",
    "\n",
    "                train_D_loss = D_L.eval(feed_dict = train_feed_dictionary)\n",
    "                train_G_loss = G_L.eval(feed_dict = train_feed_dictionary)\n",
    "                train_accuracy = accuracy.eval(feed_dict = train_feed_dictionary)\n",
    "                t_total += (time.time() - t_start)\n",
    "\n",
    "                # Debug:\n",
    "                print('train_D_loss:')\n",
    "                print(train_D_loss)\n",
    "                print('train_G_loss:')\n",
    "                print(train_G_loss)\n",
    "                print('rf_real: ')\n",
    "                print(rf_real.eval(feed_dict = train_feed_dictionary))\n",
    "                print('rf_fake: ')\n",
    "                print(rf_fake.eval(feed_dict = train_feed_dictionary))\n",
    "                print('===============')\n",
    "\n",
    "                train_D_losses.append(train_D_loss)\n",
    "                train_G_losses.append(train_G_loss)\n",
    "                train_Accs.append(train_accuracy)\n",
    "                #batch_num = batch_num + 1;\n",
    "            # Validation at the end of each epoch--BZ, Nov, 2019\n",
    "\n",
    "            \n",
    "            test_generator= get_test_batch(test_data_by_class, test_label_by_class, batch_size)\n",
    "            val_correct_preds = []\n",
    "            predictions = []\n",
    "            labels = np.zeros((0, num_classes))\n",
    "            for test_batch_x, test_batch_y in test_generator:\n",
    "            #test_batch_generator = get_test_batch(X_test, y_test, batch_size);#added--BZ, Nov 2, 2019\n",
    "            #for test_batch_x, test_batch_y in test_batch_generator:\n",
    "                val_batch_z = np.random.normal(0, 1, size = (len(test_batch_y), latent_size))\n",
    "                mask = np.ones(len(test_batch_y));#all test data is labeled   added--BZ, Nov 2, 2019\n",
    "                val_feed_dictionary = {x: test_batch_x,\n",
    "                                       z: val_batch_z,\n",
    "                                       label: test_batch_y,\n",
    "                                       labeled_mask: mask,\n",
    "                                       dropout_rate: 0.0,\n",
    "                                       is_training: False}\n",
    "\n",
    "\n",
    "                val_D_loss = D_L.eval(feed_dict = val_feed_dictionary)\n",
    "                val_G_loss = G_L.eval(feed_dict = val_feed_dictionary)\n",
    "\n",
    "                val_correct_pred = correct_prediction.eval(feed_dict = val_feed_dictionary)\n",
    "                val_correct_preds = np.concatenate((val_correct_preds, val_correct_pred))\n",
    "                    \n",
    "                val_D_real_prob = D_real_prob.eval(feed_dict = val_feed_dictionary)\n",
    "                predictions = np.concatenate((predictions, np.argmax(val_D_real_prob[:, :-1], axis = 1)))\n",
    "                labels = np.concatenate((labels, test_batch_y))\n",
    "            \n",
    "            val_accuracy = compute_val_accuracy(val_correct_preds)\n",
    "            val_Accs.append(val_accuracy)\n",
    "    \n",
    "            CM = confusion_matrix(np.argmax(labels, axis = 1), predictions, normalize='true')\n",
    "            #TPR = CM[1][1]/(CM[1][0]+CM[1][1])\n",
    "            #TNR = CM[0][0]/(CM[0][0]+CM[0][1])\n",
    "            #TPRs.append(TPR)\n",
    "            #print(val_correct_preds);\n",
    "            #print('validation_acc: %f' %(val_accuracy))\n",
    "            log_loss_acc_ternary(log_path, epoch, train_D_loss, train_G_loss, train_accuracy,\n",
    "                 val_accuracy, CM[0][0], CM[0][1], CM[0][2],\\\n",
    "                               CM[1][0], CM[1][1], CM[1][2],\\\n",
    "                              CM[2][0], CM[2][1], CM[2][2], 'w')\n",
    "    \n",
    "            save_model_recalls(model_path, sess, epoch, val_accuracy, val_Accs, CM[0][0], CM[1][1], CM[2][2])\n",
    "            \n",
    "            fakes = fake_data.eval(feed_dict = val_feed_dictionary)\n",
    "            print('fakes')\n",
    "            print(fakes)\n",
    "            #save_fake_image(fakes, epoch)\n",
    "            #confusion matrix\n",
    "            #print('epoch'+str(epoch)+' CM:')\n",
    "            #print(confusion_matrix(np.argmax(labels, axis = 1), predictions, normalize = 'true'))\n",
    "                \n",
    "        \n",
    "    return train_D_losses, train_G_losses, train_Accs, val_Accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "num_train_exs:  79\n",
      "num_val_exs:  54\n",
      "WARNING:tensorflow:From <ipython-input-15-ea1cb430e103>:6: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /Users/pengyuanzhai/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /Users/pengyuanzhai/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From <ipython-input-15-ea1cb430e103>:17: conv2d_transpose (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2DTranspose` instead.\n",
      "WARNING:tensorflow:From <ipython-input-15-ea1cb430e103>:18: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
      "tanh shape\n",
      "(None, 252, 252, 1)\n",
      "fake shape is!\n",
      "(None, 252, 252, 1)\n",
      "embed_char_expand shape is!\n",
      "(None, 252, 252, 1)\n",
      "input_x shape is!\n",
      "(None, 252, 252, 1)\n",
      "WARNING:tensorflow:From <ipython-input-14-6ded258ff857>:31: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "WARNING:tensorflow:From <ipython-input-14-6ded258ff857>:33: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "fake shape is!\n",
      "(None, 252, 252, 1)\n",
      "embed_char_expand shape is!\n",
      "(None, 252, 252, 1)\n",
      "input_x shape is!\n",
      "(None, 252, 252, 1)\n",
      "WARNING:tensorflow:From <ipython-input-19-e144679b08e3>:8: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "training....\n",
      "train_D_loss:\n",
      "2.4642444\n",
      "train_G_loss:\n",
      "0.31940573\n",
      "rf_real: \n",
      "True\n",
      "rf_fake: \n",
      "False\n",
      "===============\n",
      "fakes\n",
      "[[[[-2.68254392e-02]\n",
      "   [-2.52278056e-02]\n",
      "   [-2.49331091e-02]\n",
      "   ...\n",
      "   [-2.60034073e-02]\n",
      "   [-2.42243968e-02]\n",
      "   [-1.76554956e-02]]\n",
      "\n",
      "  [[-2.82961149e-02]\n",
      "   [-2.12410521e-02]\n",
      "   [-1.60014424e-02]\n",
      "   ...\n",
      "   [-1.02281524e-02]\n",
      "   [-2.12284680e-02]\n",
      "   [-7.64766149e-03]]\n",
      "\n",
      "  [[-2.36918312e-02]\n",
      "   [-1.29230991e-02]\n",
      "   [-1.00164097e-02]\n",
      "   ...\n",
      "   [-2.39632707e-02]\n",
      "   [-2.01094802e-02]\n",
      "   [-1.01674991e-02]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-2.83936411e-02]\n",
      "   [-1.61753111e-02]\n",
      "   [-1.62779838e-02]\n",
      "   ...\n",
      "   [-1.02087464e-02]\n",
      "   [-2.02952903e-02]\n",
      "   [-2.93120719e-03]]\n",
      "\n",
      "  [[-2.31232233e-02]\n",
      "   [-1.26383053e-02]\n",
      "   [-1.85014214e-02]\n",
      "   ...\n",
      "   [-1.34935919e-02]\n",
      "   [-1.41669083e-02]\n",
      "   [ 5.37094274e-05]]\n",
      "\n",
      "  [[-2.33483780e-03]\n",
      "   [ 1.48358252e-02]\n",
      "   [ 1.72273889e-02]\n",
      "   ...\n",
      "   [ 1.82826780e-02]\n",
      "   [ 2.05401462e-02]\n",
      "   [ 2.11535487e-02]]]\n",
      "\n",
      "\n",
      " [[[-2.21848749e-02]\n",
      "   [-3.11639849e-02]\n",
      "   [-2.83318665e-02]\n",
      "   ...\n",
      "   [-2.68417113e-02]\n",
      "   [-2.97730453e-02]\n",
      "   [-1.86998453e-02]]\n",
      "\n",
      "  [[-2.58260109e-02]\n",
      "   [-1.30873974e-02]\n",
      "   [-1.92901418e-02]\n",
      "   ...\n",
      "   [-1.42388428e-02]\n",
      "   [-1.50709925e-02]\n",
      "   [-8.81035440e-03]]\n",
      "\n",
      "  [[-2.34701335e-02]\n",
      "   [-1.26784574e-02]\n",
      "   [-2.11182050e-02]\n",
      "   ...\n",
      "   [-1.91842522e-02]\n",
      "   [-1.28855705e-02]\n",
      "   [-8.21446348e-03]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-2.58968137e-02]\n",
      "   [-1.38759036e-02]\n",
      "   [-1.52479196e-02]\n",
      "   ...\n",
      "   [-6.51246728e-03]\n",
      "   [-1.90571770e-02]\n",
      "   [-1.06449449e-03]]\n",
      "\n",
      "  [[-2.27684919e-02]\n",
      "   [-1.12420032e-02]\n",
      "   [-1.45652564e-02]\n",
      "   ...\n",
      "   [-8.78367946e-03]\n",
      "   [-1.47892497e-02]\n",
      "   [-1.07622026e-02]]\n",
      "\n",
      "  [[-5.53379359e-04]\n",
      "   [ 1.79271735e-02]\n",
      "   [ 1.52170761e-02]\n",
      "   ...\n",
      "   [ 1.90515760e-02]\n",
      "   [ 1.61068179e-02]\n",
      "   [ 1.60359032e-02]]]\n",
      "\n",
      "\n",
      " [[[-2.44714953e-02]\n",
      "   [-2.90028397e-02]\n",
      "   [-2.78286673e-02]\n",
      "   ...\n",
      "   [-3.00426539e-02]\n",
      "   [-2.58191954e-02]\n",
      "   [-1.97987966e-02]]\n",
      "\n",
      "  [[-2.29754504e-02]\n",
      "   [-1.41161280e-02]\n",
      "   [-2.30690036e-02]\n",
      "   ...\n",
      "   [-7.97517365e-04]\n",
      "   [-1.13665443e-02]\n",
      "   [-4.71646339e-03]]\n",
      "\n",
      "  [[-2.12658141e-02]\n",
      "   [-1.81532800e-02]\n",
      "   [-1.37634063e-02]\n",
      "   ...\n",
      "   [-1.63406655e-02]\n",
      "   [-2.03161649e-02]\n",
      "   [-7.45842466e-03]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-2.20647678e-02]\n",
      "   [-1.41634680e-02]\n",
      "   [-1.41079659e-02]\n",
      "   ...\n",
      "   [-1.52844116e-02]\n",
      "   [-1.43752499e-02]\n",
      "   [-5.30961249e-03]]\n",
      "\n",
      "  [[-2.27409396e-02]\n",
      "   [-2.12527439e-02]\n",
      "   [-2.05932707e-02]\n",
      "   ...\n",
      "   [-1.81093439e-02]\n",
      "   [-2.14820392e-02]\n",
      "   [-7.46192830e-03]]\n",
      "\n",
      "  [[-9.85068502e-04]\n",
      "   [ 1.80310979e-02]\n",
      "   [ 1.61195174e-02]\n",
      "   ...\n",
      "   [ 2.04498302e-02]\n",
      "   [ 1.53906457e-02]\n",
      "   [ 1.36143975e-02]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[-2.56748721e-02]\n",
      "   [-2.66686454e-02]\n",
      "   [-2.81856917e-02]\n",
      "   ...\n",
      "   [-3.01593170e-02]\n",
      "   [-2.80965026e-02]\n",
      "   [-1.93403549e-02]]\n",
      "\n",
      "  [[-2.25171410e-02]\n",
      "   [-1.45678418e-02]\n",
      "   [-1.87031869e-02]\n",
      "   ...\n",
      "   [-9.95199662e-03]\n",
      "   [-1.84566248e-02]\n",
      "   [-6.09461975e-04]]\n",
      "\n",
      "  [[-2.53153555e-02]\n",
      "   [-1.56902708e-02]\n",
      "   [-1.50029548e-02]\n",
      "   ...\n",
      "   [-1.48062026e-02]\n",
      "   [-1.60977412e-02]\n",
      "   [-5.34275174e-03]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-2.45291442e-02]\n",
      "   [-1.19486731e-02]\n",
      "   [-1.78829003e-02]\n",
      "   ...\n",
      "   [-1.02239344e-02]\n",
      "   [-1.14342915e-02]\n",
      "   [-7.40449317e-03]]\n",
      "\n",
      "  [[-2.47545261e-02]\n",
      "   [-1.81280319e-02]\n",
      "   [-1.78191047e-02]\n",
      "   ...\n",
      "   [-1.28278984e-02]\n",
      "   [-1.28303189e-02]\n",
      "   [-4.02761856e-03]]\n",
      "\n",
      "  [[ 1.84899822e-04]\n",
      "   [ 1.76218767e-02]\n",
      "   [ 1.90706588e-02]\n",
      "   ...\n",
      "   [ 1.86935775e-02]\n",
      "   [ 2.09054053e-02]\n",
      "   [ 1.69986747e-02]]]\n",
      "\n",
      "\n",
      " [[[-2.43972577e-02]\n",
      "   [-2.76303757e-02]\n",
      "   [-2.85508484e-02]\n",
      "   ...\n",
      "   [-3.09551563e-02]\n",
      "   [-2.39640754e-02]\n",
      "   [-1.78168453e-02]]\n",
      "\n",
      "  [[-2.36027222e-02]\n",
      "   [-1.40175167e-02]\n",
      "   [-1.93818510e-02]\n",
      "   ...\n",
      "   [-8.64240434e-03]\n",
      "   [-1.86864715e-02]\n",
      "   [-4.38237377e-03]]\n",
      "\n",
      "  [[-2.60399152e-02]\n",
      "   [-1.20589854e-02]\n",
      "   [-1.68725997e-02]\n",
      "   ...\n",
      "   [-1.66724790e-02]\n",
      "   [-1.84145588e-02]\n",
      "   [-2.14858004e-03]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-2.64187977e-02]\n",
      "   [-1.51089719e-02]\n",
      "   [-2.28505712e-02]\n",
      "   ...\n",
      "   [-1.51071120e-02]\n",
      "   [-1.63185596e-02]\n",
      "   [ 5.65979513e-04]]\n",
      "\n",
      "  [[-2.01638788e-02]\n",
      "   [-2.15989277e-02]\n",
      "   [-1.74266547e-02]\n",
      "   ...\n",
      "   [-1.48893502e-02]\n",
      "   [-1.68700926e-02]\n",
      "   [-6.38764817e-03]]\n",
      "\n",
      "  [[-1.96890323e-03]\n",
      "   [ 1.92600563e-02]\n",
      "   [ 1.93033945e-02]\n",
      "   ...\n",
      "   [ 2.24275403e-02]\n",
      "   [ 1.39150508e-02]\n",
      "   [ 1.49268368e-02]]]\n",
      "\n",
      "\n",
      " [[[-2.49889307e-02]\n",
      "   [-2.69859713e-02]\n",
      "   [-2.97653507e-02]\n",
      "   ...\n",
      "   [-3.09849605e-02]\n",
      "   [-2.94723026e-02]\n",
      "   [-2.23714467e-02]]\n",
      "\n",
      "  [[-2.69498546e-02]\n",
      "   [-1.29213845e-02]\n",
      "   [-2.23401450e-02]\n",
      "   ...\n",
      "   [-1.06962919e-02]\n",
      "   [-2.11950820e-02]\n",
      "   [-2.95832334e-03]]\n",
      "\n",
      "  [[-2.52251029e-02]\n",
      "   [-1.93750858e-02]\n",
      "   [-1.51122920e-02]\n",
      "   ...\n",
      "   [-1.52389053e-02]\n",
      "   [-1.73154231e-02]\n",
      "   [-5.98771079e-03]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-2.80466769e-02]\n",
      "   [-1.41972350e-02]\n",
      "   [-1.19484635e-02]\n",
      "   ...\n",
      "   [-3.58529692e-03]\n",
      "   [-2.43644230e-02]\n",
      "   [-7.54849287e-03]]\n",
      "\n",
      "  [[-2.34869309e-02]\n",
      "   [-1.37489270e-02]\n",
      "   [-1.59848649e-02]\n",
      "   ...\n",
      "   [-1.18434299e-02]\n",
      "   [-1.66090187e-02]\n",
      "   [-3.64434742e-03]]\n",
      "\n",
      "  [[ 4.17088042e-04]\n",
      "   [ 1.80451907e-02]\n",
      "   [ 1.94596425e-02]\n",
      "   ...\n",
      "   [ 1.64340511e-02]\n",
      "   [ 1.87119786e-02]\n",
      "   [ 1.76639184e-02]]]]\n",
      "train_D_loss:\n",
      "1.7565879\n",
      "train_G_loss:\n",
      "0.32850355\n",
      "rf_real: \n",
      "True\n",
      "rf_fake: \n",
      "False\n",
      "===============\n",
      "fakes\n",
      "[[[[-0.03826236]\n",
      "   [-0.05874278]\n",
      "   [-0.06439915]\n",
      "   ...\n",
      "   [-0.05923018]\n",
      "   [-0.06409303]\n",
      "   [-0.04072804]]\n",
      "\n",
      "  [[-0.04979953]\n",
      "   [-0.0311154 ]\n",
      "   [-0.03962175]\n",
      "   ...\n",
      "   [-0.02944589]\n",
      "   [-0.04010684]\n",
      "   [-0.00763513]]\n",
      "\n",
      "  [[-0.05167365]\n",
      "   [-0.03172145]\n",
      "   [-0.0368205 ]\n",
      "   ...\n",
      "   [-0.03734732]\n",
      "   [-0.03936806]\n",
      "   [-0.00524115]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.05685594]\n",
      "   [-0.04153224]\n",
      "   [-0.03202727]\n",
      "   ...\n",
      "   [-0.02481451]\n",
      "   [-0.03526777]\n",
      "   [-0.0025056 ]]\n",
      "\n",
      "  [[-0.04913421]\n",
      "   [-0.02867091]\n",
      "   [-0.0390718 ]\n",
      "   ...\n",
      "   [-0.04086395]\n",
      "   [-0.02988289]\n",
      "   [-0.01203989]]\n",
      "\n",
      "  [[-0.00448642]\n",
      "   [ 0.03763358]\n",
      "   [ 0.03502773]\n",
      "   ...\n",
      "   [ 0.0424597 ]\n",
      "   [ 0.03971499]\n",
      "   [ 0.03139697]]]\n",
      "\n",
      "\n",
      " [[[-0.05050904]\n",
      "   [-0.05587348]\n",
      "   [-0.05607766]\n",
      "   ...\n",
      "   [-0.0570527 ]\n",
      "   [-0.05995808]\n",
      "   [-0.03779025]]\n",
      "\n",
      "  [[-0.05283125]\n",
      "   [-0.0363843 ]\n",
      "   [-0.03376788]\n",
      "   ...\n",
      "   [-0.02846244]\n",
      "   [-0.04153053]\n",
      "   [-0.00944228]]\n",
      "\n",
      "  [[-0.05616503]\n",
      "   [-0.02980074]\n",
      "   [-0.02581095]\n",
      "   ...\n",
      "   [-0.03548462]\n",
      "   [-0.03719445]\n",
      "   [-0.00812053]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.05129048]\n",
      "   [-0.02634437]\n",
      "   [-0.0305013 ]\n",
      "   ...\n",
      "   [-0.02101974]\n",
      "   [-0.04077039]\n",
      "   [-0.01718224]]\n",
      "\n",
      "  [[-0.05079635]\n",
      "   [-0.03543252]\n",
      "   [-0.03406713]\n",
      "   ...\n",
      "   [-0.02682568]\n",
      "   [-0.02426993]\n",
      "   [-0.01540975]]\n",
      "\n",
      "  [[ 0.0036003 ]\n",
      "   [ 0.03793122]\n",
      "   [ 0.03919384]\n",
      "   ...\n",
      "   [ 0.03335593]\n",
      "   [ 0.04103889]\n",
      "   [ 0.03400248]]]\n",
      "\n",
      "\n",
      " [[[-0.04759657]\n",
      "   [-0.06046371]\n",
      "   [-0.060512  ]\n",
      "   ...\n",
      "   [-0.05846122]\n",
      "   [-0.05131325]\n",
      "   [-0.04357789]]\n",
      "\n",
      "  [[-0.05286371]\n",
      "   [-0.03116903]\n",
      "   [-0.04347068]\n",
      "   ...\n",
      "   [-0.03453858]\n",
      "   [-0.03761031]\n",
      "   [-0.00688966]]\n",
      "\n",
      "  [[-0.05245326]\n",
      "   [-0.04294277]\n",
      "   [-0.0332494 ]\n",
      "   ...\n",
      "   [-0.03584874]\n",
      "   [-0.03326518]\n",
      "   [-0.00971773]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.050033  ]\n",
      "   [-0.02456926]\n",
      "   [-0.02895154]\n",
      "   ...\n",
      "   [-0.01783722]\n",
      "   [-0.03821925]\n",
      "   [-0.00967798]]\n",
      "\n",
      "  [[-0.05149078]\n",
      "   [-0.03009135]\n",
      "   [-0.03294113]\n",
      "   ...\n",
      "   [-0.02762068]\n",
      "   [-0.02937275]\n",
      "   [-0.01837729]]\n",
      "\n",
      "  [[ 0.00362257]\n",
      "   [ 0.03969194]\n",
      "   [ 0.03580973]\n",
      "   ...\n",
      "   [ 0.02931903]\n",
      "   [ 0.03705239]\n",
      "   [ 0.036502  ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[-0.04768929]\n",
      "   [-0.05493751]\n",
      "   [-0.05980195]\n",
      "   ...\n",
      "   [-0.0578107 ]\n",
      "   [-0.06143049]\n",
      "   [-0.04122607]]\n",
      "\n",
      "  [[-0.05198977]\n",
      "   [-0.02712622]\n",
      "   [-0.03576224]\n",
      "   ...\n",
      "   [-0.02753064]\n",
      "   [-0.03458365]\n",
      "   [-0.00679921]]\n",
      "\n",
      "  [[-0.04646633]\n",
      "   [-0.0351818 ]\n",
      "   [-0.03484374]\n",
      "   ...\n",
      "   [-0.03421907]\n",
      "   [-0.03168529]\n",
      "   [-0.01476988]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.04891437]\n",
      "   [-0.02552955]\n",
      "   [-0.03356975]\n",
      "   ...\n",
      "   [-0.0171423 ]\n",
      "   [-0.04250813]\n",
      "   [-0.01801376]]\n",
      "\n",
      "  [[-0.04891112]\n",
      "   [-0.03412861]\n",
      "   [-0.02827345]\n",
      "   ...\n",
      "   [-0.03038842]\n",
      "   [-0.0370938 ]\n",
      "   [-0.02177219]]\n",
      "\n",
      "  [[ 0.00399563]\n",
      "   [ 0.04228353]\n",
      "   [ 0.03603157]\n",
      "   ...\n",
      "   [ 0.0352303 ]\n",
      "   [ 0.02851029]\n",
      "   [ 0.03904558]]]\n",
      "\n",
      "\n",
      " [[[-0.05236322]\n",
      "   [-0.05956967]\n",
      "   [-0.05645016]\n",
      "   ...\n",
      "   [-0.06801546]\n",
      "   [-0.06648257]\n",
      "   [-0.04100255]]\n",
      "\n",
      "  [[-0.06030387]\n",
      "   [-0.02674543]\n",
      "   [-0.03392766]\n",
      "   ...\n",
      "   [-0.02530224]\n",
      "   [-0.03481568]\n",
      "   [-0.00943188]]\n",
      "\n",
      "  [[-0.05311248]\n",
      "   [-0.02743847]\n",
      "   [-0.02780656]\n",
      "   ...\n",
      "   [-0.02771926]\n",
      "   [-0.03468136]\n",
      "   [-0.01030517]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.05546928]\n",
      "   [-0.01881394]\n",
      "   [-0.04610259]\n",
      "   ...\n",
      "   [-0.03315882]\n",
      "   [-0.0449728 ]\n",
      "   [-0.01777469]]\n",
      "\n",
      "  [[-0.05352082]\n",
      "   [-0.03061366]\n",
      "   [-0.0318516 ]\n",
      "   ...\n",
      "   [-0.03993887]\n",
      "   [-0.04522349]\n",
      "   [-0.0073654 ]]\n",
      "\n",
      "  [[-0.00188003]\n",
      "   [ 0.03620346]\n",
      "   [ 0.03100553]\n",
      "   ...\n",
      "   [ 0.03213248]\n",
      "   [ 0.04379984]\n",
      "   [ 0.03923409]]]\n",
      "\n",
      "\n",
      " [[[-0.04757124]\n",
      "   [-0.05968575]\n",
      "   [-0.06174866]\n",
      "   ...\n",
      "   [-0.05286261]\n",
      "   [-0.05963027]\n",
      "   [-0.04077707]]\n",
      "\n",
      "  [[-0.05366828]\n",
      "   [-0.03361157]\n",
      "   [-0.03608778]\n",
      "   ...\n",
      "   [-0.02331722]\n",
      "   [-0.02764711]\n",
      "   [-0.00639698]]\n",
      "\n",
      "  [[-0.04049789]\n",
      "   [-0.02964672]\n",
      "   [-0.03127212]\n",
      "   ...\n",
      "   [-0.0318853 ]\n",
      "   [-0.03875991]\n",
      "   [-0.00584003]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.0464731 ]\n",
      "   [-0.02895875]\n",
      "   [-0.03657386]\n",
      "   ...\n",
      "   [-0.02348494]\n",
      "   [-0.02797721]\n",
      "   [-0.0071993 ]]\n",
      "\n",
      "  [[-0.05301749]\n",
      "   [-0.03877948]\n",
      "   [-0.04217912]\n",
      "   ...\n",
      "   [-0.03902008]\n",
      "   [-0.04370477]\n",
      "   [-0.02274068]]\n",
      "\n",
      "  [[ 0.00177525]\n",
      "   [ 0.03592066]\n",
      "   [ 0.03444439]\n",
      "   ...\n",
      "   [ 0.0426538 ]\n",
      "   [ 0.03406202]\n",
      "   [ 0.02949121]]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_D_loss:\n",
      "1.5977111\n",
      "train_G_loss:\n",
      "0.34243867\n",
      "rf_real: \n",
      "True\n",
      "rf_fake: \n",
      "False\n",
      "===============\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-c55c011adcbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_SSL_GAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m340\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_by_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label_by_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_by_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_label_by_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-d328f36a2cdb>\u001b[0m in \u001b[0;36mtrain_SSL_GAN\u001b[0;34m(batch_size, num_epochs, train_data_by_class, train_label_by_class, test_data_by_class, test_label_by_class)\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0mval_G_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG_L\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_feed_dictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m                 \u001b[0mval_correct_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrect_prediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_feed_dictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m                 \u001b[0mval_correct_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_correct_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_correct_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m     \"\"\"\n\u001b[0;32m--> 798\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mexperimental_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   5408\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5409\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 5410\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_SSL_GAN(60, 340, train_data_by_class, train_label_by_class, test_data_by_class, test_label_by_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
